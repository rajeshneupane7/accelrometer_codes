{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Suppress warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# ==========================================\n",
    "# 1. ADAPTIVE PIPELINE (Handles Mixed Hz)\n",
    "# ==========================================\n",
    "class AdaptiveAccelPipeline:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df['local_ts'] = pd.to_datetime(self.df['local_ts'])\n",
    "        # Sort is critical for time-series\n",
    "        self.df = self.df.sort_values(by=['subject', 'local_ts']).reset_index(drop=True)\n",
    "\n",
    "    def apply_adaptive_filter(self, target_cutoff_hz=5.0):\n",
    "        """\n",
    "        Applies filtering subject-by-subject.\n",
    "        If a subject's sampling rate is too low, it auto-adjusts or skips filtering.\n",
    "        """\n",
    "        print(f\"--- Running Adaptive Filter (Target Cutoff: {target_cutoff_hz}Hz) ---\")\n",
    "        \n",
    "        def filter_subject_data(group):\n",
    "            # 1. Calculate Sampling Rate (fs) for THIS subject\n",
    "            # We filter out 0.0 diffs (duplicates) to avoid divide-by-zero\n",
    "            time_diffs = group['local_ts'].diff().dt.total_seconds()\n",
    "            valid_diffs = time_diffs[time_diffs > 0]\n",
    "            \n",
    "            if len(valid_diffs) < 10: return group # Too little data to process\n",
    "            \n",
    "            median_diff = valid_diffs.median()\n",
    "            if median_diff == 0: return group # Safety catch\n",
    "            \n",
    "            fs = 1 / median_diff\n",
    "            nyquist = 0.5 * fs\n",
    "            \n",
    "            # 2. DECISION LOGIC\n",
    "            # Case A: Data is too slow for the requested filter (e.g., Data is 5Hz, you want 5Hz cut)\n",
    "            if fs <= target_cutoff_hz * 2.0:\n",
    "                # If data is really slow (<10Hz), usually we just skip filtering \n",
    "                # because it's already \"smooth\" compared to 20Hz data.\n",
    "                # Or we apply a very mild smoothing (e.g. 0.8 * Nyquist)\n",
    "                actual_cutoff = nyquist * 0.9 \n",
    "                \n",
    "                # If the resulting cutoff is tiny, just skip it.\n",
    "                if actual_cutoff < 1.0: return group \n",
    "            else:\n",
    "                # Case B: Data is fast enough (e.g., 20Hz data, 5Hz cut) -> Use requested cutoff\n",
    "                actual_cutoff = target_cutoff_hz\n",
    "\n",
    "            # 3. Apply Filter\n",
    "            try:\n",
    "                b, a = butter(N=4, Wn=actual_cutoff / nyquist, btype='low')\n",
    "                # Apply to X, Y, Z\n",
    "                for col in ['x', 'y', 'z']:\n",
    "                    group[col] = filtfilt(b, a, group[col])\n",
    "            except Exception as e: # If math fails (e.g., unstable IIR), return raw data\n",
    "                pass\n",
    "                \n",
    "            return group\n",
    "\n",
    "        # Apply the logic to each subject independently\n",
    "        self.df = self.df.groupby('subject', group_keys=False).apply(filter_subject_data)\n",
    "        print(\"--- Adaptive Filtering Complete ---\")\n",
    "        return self.df\n",
    "\n",
    "    def convert_to_gravity(self):\n",
    "        print(\"--- Converting to Gravity & ENMO ---\")\n",
    "        scale = 16384.0 # Adjust based on your accelerometer range (e.g. +/- 2g vs +/- 4g)\n",
    "        self.df['x_g'] = self.df['x'] / scale\n",
    "        self.df['y_g'] = self.df['y'] / scale\n",
    "        self.df['z_g'] = self.df['z'] / scale\n",
    "        self.df['mag'] = np.sqrt(self.df['x_g']**2 + self.df['y_g']**2 + self.df['z_g']**2)\n",
    "        self.df['enmo'] = np.maximum(self.df['mag'] - 1, 0)\n",
    "        return self.df\n",
    "\n",
    "    def calc_odba(self):\n",
    "        # Simplified ODBA calculation\n",
    "        self.df['odba'] = (self.df['x_g'] - self.df['x_g'].mean()).abs() + \n",
    "                          (self.df['y_g'] - self.df['y_g'].mean()).abs() + \n",
    "                          (self.df['z_g'] - self.df['z_g'].mean()).abs()\n",
    "        return self.df\n",
    "\n",
    "    def resample_and_label(self, interval_seconds=10, coherence_threshold=0.7):\n",
    "        """\n",
    "        Resamples data into windows. \n",
    "        ASSIGN LABELS based on threshold:\n",
    "        If > 70% of the raw samples in the window are 'Grazing', the window is 'Grazing'.\n",
    "        Otherwise, the window is discarded (Ambiguous).\n",
    "        """\n",
    "        print(f\"--- Resampling ({interval_seconds}s) with Threshold {coherence_threshold*100}% ---\")\n",
    "        \n",
    "        # Custom Aggregator for Labels\n",
    "        def threshold_labeler(x):\n",
    "            if x.empty: return np.nan\n",
    "            counts = x.value_counts(normalize=True)\n",
    "            # Check if the most frequent label crosses the threshold\n",
    "            if counts.iloc[0] >= coherence_threshold:\n",
    "                return counts.index[0]\n",
    "            return np.nan # Drop this window (too messy/transitioning)\n",
    "\n",
    "        # Feature Aggregators\n",
    "        agg_dict = {\n",
    "            'x_g': ['mean', 'std', 'min', 'max'],\n",
    "            'y_g': ['mean', 'std', 'min', 'max'],\n",
    "            'z_g': ['mean', 'std', 'min', 'max'],\n",
    "            'mag': ['mean', 'std'],      \n",
    "            'enmo': ['mean', 'max'], \n",
    "            'odba': ['mean', 'std'],    \n",
    "            'behavioral_category': threshold_labeler # <--- LOGIC APPLIED HERE\n",
    "        }\n",
    "\n",
    "        resampled = (\n",
    "            self.df.set_index('local_ts')\n",
    "            .groupby('subject')\n",
    "            .resample(f'{interval_seconds}s')\n",
    "            .agg(agg_dict)\n",
    "        )\n",
    "        \n",
    "        # Flatten columns\n",
    "        resampled.columns = [f\"{c[0]}_{c[1]}\" if c[1] else c[0] for c in resampled.columns]\n",
    "        resampled = resampled.rename(columns={'behavioral_category_threshold_labeler': 'label'})\n",
    "        \n",
    "        # DROP windows that failed the threshold check\n",
    "        final_df = resampled.dropna(subset=['label']).reset_index()\n",
    "        \n",
    "        print(f\"Generated {len(final_df)} labeled windows.\")\n",
    "        return final_df\n",
    "\n",
    "    def create_time_series_sequences(self, data, feature_cols, target_col, time_steps=5):\n",
    "        """\n",
    "        Converts the resampled windows into sequences for LSTM.\n",
    "        Input: (N, Features) -> Output: (N, TimeSteps, Features)\n",
    "        """\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        # Group by subject to prevent sequences bleeding across different animals\n",
    "        for subject, group in data.groupby('subject'):\n",
    "            group = group.sort_values('local_ts')\n",
    "            feats = group[feature_cols].values\n",
    "            targets = group[target_col].values\n",
    "            \n",
    "            if len(group) < time_steps: continue\n",
    "            \n",
    "            # Sliding window over the WINDOWS\n",
    "            for i in range(len(group) - time_steps):\n",
    "                X_seq.append(feats[i : i + time_steps])\n",
    "                y_seq.append(targets[i + time_steps]) # Predict the label of the *next* window\n",
    "                \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEEP LEARNING ENGINE (PyTorch)\n",
    "# ==========================================\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class TimeSeriesModeler:\n",
    "    def __init__(self, pipeline, df):\n",
    "        self.pipeline = pipeline\n",
    "        self.df = df\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "    def run(self, time_steps=5):\n",
    "        print(f\"\n>>> Running Time Series Model (Lookback: {time_steps} steps)\")\n",
    "        \n",
    "        # 1. Identify Feature Columns (exclude metadata)\n",
    "        feature_cols = [c for c in self.df.columns if c not in ['subject', 'local_ts', 'label']]\n",
    "        \n",
    "        # 2. Encode Labels\n",
    "        self.df['label_encoded'] = self.le.fit_transform(self.df['label'])\n",
    "        num_classes = len(self.le.classes_)\n",
    "        \n",
    "        # 3. Scale Features\n",
    "        scaler = StandardScaler()\n",
    "        scaled_df = self.df.copy()\n",
    "        scaled_df[feature_cols] = scaler.fit_transform(scaled_df[feature_cols])\n",
    "        \n",
    "        # 4. Generate Sequences\n",
    "        X, y = self.pipeline.create_time_series_sequences(\n",
    "            scaled_df, feature_cols, 'label_encoded', time_steps\n",
    "        )\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"Not enough data for sequences.\")\n",
    "            return None\n",
    "\n",
    "        # 5. Train/Test Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # 6. Get Test Set Class Counts\n",
    "        class_indices, counts = np.unique(y_test, return_counts=True)\n",
    "        class_names = self.le.inverse_transform(class_indices)\n",
    "        test_counts = str(dict(zip(class_names, counts.astype(int)))) # Store as string\n",
    "\n",
    "        # 7. Convert to PyTorch Tensors\n",
    "        X_train_tensor = torch.from_numpy(X_train).float()\n",
    "        y_train_tensor = torch.from_numpy(y_train).long()\n",
    "        X_test_tensor = torch.from_numpy(X_test).float()\n",
    "        y_test_tensor = torch.from_numpy(y_test).long()\n",
    "        \n",
    "        # 8. Create DataLoaders\n",
    "        train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_data, shuffle=True, batch_size=32, drop_last=True)\n",
    "        test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        test_loader = DataLoader(test_data, batch_size=32)\n",
    "        \n",
    "        # 9. Initialize Model, Loss, and Optimizer\n",
    "        model = LSTMModel(input_dim=X_train.shape[2], hidden_dim=64, output_dim=num_classes, n_layers=1)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "        \n",
    "        # 10. Training Loop with Early Stopping\n",
    "        epochs = 20\n",
    "        patience = 5\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "        best_model_wts = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                    val_losses.append(val_loss.item())\n",
    "            \n",
    "            epoch_val_loss = np.mean(val_losses)\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            if epochs_no_improve >= patience:\n",
    "                model.load_state_dict(best_model_wts)\n",
    "                break\n",
    "                \n",
    "        # 11. Evaluate\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.numpy())\n",
    "                all_true.extend(labels.numpy())\n",
    "        \n",
    "        acc = accuracy_score(all_true, all_preds)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(all_true, all_preds, average='weighted', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'Accuracy': acc,\n",
    "            'F1_Score': f1,\n",
    "            'Time_Steps': time_steps,\n",
    "            'Num_Training_Sequences': len(X_train),\n",
    "            'Test_Set_Counts': test_counts\n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# A. LOAD DATA\n",
    "actual_file = []\n",
    "main_path = \"/home/rajesh/work/acclerometer_project/zip_data\"\n",
    "\n",
    "if os.path.exists(main_path):\n",
    "    for zip_file in os.listdir(main_path):\n",
    "        if zip_file.endswith(\".zip\"):\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                try:\n",
    "                    with zipfile.ZipFile(os.path.join(main_path, zip_file), \"r\") as zf:\n",
    "                        zf.extractall(temp_dir)\n",
    "                        for root, dirs, files in os.walk(temp_dir):\n",
    "                            for d in dirs:\n",
    "                                if d.startswith('Processed'):\n",
    "                                    sec_path = os.path.join(root, d)\n",
    "                                    for f in os.listdir(sec_path):\n",
    "                                        if f.endswith((',xls', '.xlsx')):\n",
    "                                            actual_file.append(pd.read_excel(os.path.join(sec_path, f)))\n",
    "                except Exception as e: print(f\"Error: {e}\")\n",
    "    if actual_file: df_raw = pd.concat(actual_file)\n",
    "    else: df_raw = pd.DataFrame() # Handle empty case\n",
    "else:\n",
    "    # Dummy data for testing if path doesn't exist\n",
    "    print(\"Using Dummy Data\")\n",
    "    dates = pd.date_range('2023-01-01', periods=5000, freq='200ms') # 5Hz data\n",
    "    df_raw = pd.DataFrame({\n",
    "        'subject': ['Cow1']*5000,\n",
    "        'local_ts': dates,\n",
    "        'x': np.random.randn(5000)*1000, \n",
    "        'y': np.random.randn(5000)*1000,\n",
    "        'z': np.random.randn(5000)*1000,\n",
    "        'behavioral_category': ['Grazing']*2500 + ['Resting']*2500 \n",
    "    })\n",
    "\n",
    "if not df_raw.empty:\n",
    "    # B. INITIALIZE ADAPTIVE PIPELINE\n",
    "    pipeline = AdaptiveAccelPipeline(df_raw)\n",
    "\n",
    "    # 1. Adaptively Filter (safe for mixed 5Hz/10Hz/20Hz)\n",
    "    pipeline.apply_adaptive_filter(target_cutoff_hz=5.0)\n",
    "\n",
    "    # 2. Physics conversions\n",
    "    pipeline.convert_to_gravity()\n",
    "    pipeline.calc_odba()\n",
    "\n",
    "    results_table = []\n",
    "    \n",
    "    # Grid Search Settings\n",
    "    intervals = [10, 30]  # Window sizes in seconds\n",
    "    thresholds = [0.6, 0.8] # Strictness of labeling (60% vs 80% purity)\n",
    "    lstm_steps = [5, 10]    # How far back the LSTM looks\n",
    "\n",
    "    print(f\"\n--- Starting Grid Search ---\")\n",
    "    for iv in intervals:\n",
    "        for th in thresholds:\n",
    "            # 3. Resample and Label\n",
    "            # This applies the logic: \"Only keep window if > X% of data matches\"\n",
    "            df_ready = pipeline.resample_and_label(interval_seconds=iv, coherence_threshold=th)\n",
    "            \n",
    "            if len(df_ready) < 100:\n",
    "                print(f\"Skipping Interval={iv}, Threshold={th} (Not enough valid windows)\")\n",
    "                continue\n",
    "\n",
    "            # 4. Run Time Series Model\n",
    "            ts_modeler = TimeSeriesModeler(pipeline, df_ready)\n",
    "            \n",
    "            for steps in lstm_steps:\n",
    "                res = ts_modeler.run(time_steps=steps)\n",
    "                if res:\n",
    "                    res['Window_Size_Sec'] = iv\n",
    "                    res['Label_Threshold'] = th\n",
    "                    results_table.append(res)\n",
    "\n",
    "    # C. SAVE RESULTS\n",
    "    if results_table:\n",
    "        final_df = pd.DataFrame(results_table)\n",
    "        final_df = final_df.sort_values(by='F1_Score', ascending=False)\n",
    "        \n",
    "        print(\"\n=== FINAL RESULTS ===\")\n",
    "        print(final_df)\n",
    "        \n",
    "        save_path = '/home/rajesh/work/acclerometer_project/codes/timeseries_results.csv'\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        final_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"No results generated.\")\n",
    "else:\n",
    "    print(\"No Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ced191",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf46f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heat_stress_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}