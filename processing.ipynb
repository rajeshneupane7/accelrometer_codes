{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e406ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA PROCESSING PIPELINE\n",
    "# ==========================================\n",
    "class AccelPipeline:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "        # Ensure Timestamp format\n",
    "        self.df['local_ts'] = pd.to_datetime(self.df['local_ts'])\n",
    "        \n",
    "        # Sort by Subject THEN Time to ensure rolling window correctness\n",
    "        self.df = self.df.sort_values(by=['subject', 'local_ts']).reset_index(drop=True)\n",
    "        \n",
    "    def convert_to_gravity(self):\n",
    "        print(\"--- Converting to Gravity Units & Calculating ENMO ---\")\n",
    "        scale = 16384.0\n",
    "        \n",
    "        self.df['x_g'] = self.df['x'] / scale\n",
    "        self.df['y_g'] = self.df['y'] / scale\n",
    "        self.df['z_g'] = self.df['z'] / scale\n",
    "        \n",
    "        # Magnitude\n",
    "        self.df['mag'] = np.sqrt(self.df['x_g']**2 + self.df['y_g']**2 + self.df['z_g']**2)\n",
    "        \n",
    "        # ENMO: max(mag - 1, 0)\n",
    "        self.df['enmo'] = np.maximum(self.df['mag'] - 1, 0)\n",
    "        return self.df\n",
    "\n",
    "    def _get_dynamic_component(self, window_seconds=1):\n",
    "        # Create temp df indexed by time\n",
    "        temp_df = self.df.set_index('local_ts').sort_index()\n",
    "        cols = ['x_g', 'y_g', 'z_g']\n",
    "        \n",
    "        # Group by subject -> Rolling Mean (Static Gravity)\n",
    "        static_component = temp_df.groupby('subject')[cols].rolling(f'{window_seconds}s').mean()\n",
    "        \n",
    "        # Merge static values back to main dataframe\n",
    "        static_reset = static_component.reset_index()\n",
    "        merged = pd.merge(self.df, static_reset, on=['subject', 'local_ts'], suffixes=('', '_static'))\n",
    "        \n",
    "        # Dynamic = Raw - Static\n",
    "        dynamic_df = pd.DataFrame()\n",
    "        dynamic_df['x_d'] = merged['x_g'] - merged['x_g_static']\n",
    "        dynamic_df['y_d'] = merged['y_g'] - merged['y_g_static']\n",
    "        dynamic_df['z_d'] = merged['z_g'] - merged['z_g_static']\n",
    "        \n",
    "        return dynamic_df.fillna(0)\n",
    "\n",
    "    def calc_odba(self):\n",
    "        print(\"--- Calculating ODBA ---\")\n",
    "        dyn = self._get_dynamic_component()\n",
    "        self.df['odba'] = dyn['x_d'].abs() + dyn['y_d'].abs() + dyn['z_d'].abs()\n",
    "        return self.df\n",
    "\n",
    "    def calc_vedba(self):\n",
    "        print(\"--- Calculating VeDBA ---\")\n",
    "        dyn = self._get_dynamic_component()\n",
    "        self.df['vedba'] = np.sqrt(dyn['x_d']**2 + dyn['y_d']**2 + dyn['z_d']**2)\n",
    "        return self.df\n",
    "\n",
    "    def resample_data(self, interval_seconds=5):\n",
    "        print(f\"--- Resampling data to {interval_seconds} second windows ---\")\n",
    "        \n",
    "        # Aggregation Dictionary\n",
    "        agg_dict = {\n",
    "            'x_g': 'mean', 'y_g': 'mean', 'z_g': 'mean',\n",
    "            'mag': 'mean', 'enmo': 'mean',\n",
    "            'odba': 'mean', 'vedba': 'mean',\n",
    "            # UPDATED: Mode for behavioral_category\n",
    "            'behavioral_category': lambda x: x.mode()[0] if not x.mode().empty else np.nan\n",
    "        }\n",
    "        \n",
    "        resampled_df = (\n",
    "            self.df.set_index('local_ts')\n",
    "            .groupby('subject')\n",
    "            .resample(f'{interval_seconds}s')\n",
    "            .agg(agg_dict)\n",
    "        )\n",
    "        \n",
    "        return resampled_df.dropna().reset_index()\n",
    "\n",
    "# ==========================================\n",
    "# 2. MACHINE LEARNING MODELER\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f45379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class ActivityModeler:\n",
    "    def __init__(self, data, target_col='behavioral_category'):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "        # Store results here\n",
    "        self.results_log = []\n",
    "        \n",
    "        # Check target\n",
    "        if self.target_col not in self.data.columns:\n",
    "            raise ValueError(f\"Target column '{self.target_col}' not found.\")\n",
    "            \n",
    "        # Capture Value Counts as a string for the report\n",
    "        counts = self.data[self.target_col].value_counts().to_dict()\n",
    "        self.target_distribution_str = str(counts)\n",
    "\n",
    "    def prepare_data(self, feature_cols):\n",
    "        \"\"\"Prepares X and y, applies scaling for non-tree models.\"\"\"\n",
    "        X = self.data[feature_cols]\n",
    "        y = self.data[self.target_col]\n",
    "        \n",
    "        # Encode labels\n",
    "        y_encoded = self.le.fit_transform(y)\n",
    "        \n",
    "        # Split\n",
    "        # Try stratify, fallback if classes are too small\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
    "            )\n",
    "        except ValueError:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y_encoded, test_size=0.3, random_state=42\n",
    "            )\n",
    "            \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _evaluate_model(self, model, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"Fits model and calculates metrics.\"\"\"\n",
    "        # Scale data for algorithms that need it (SVM, KNN, LogReg)\n",
    "        # Random Forest doesn't strictly need it, but it doesn't hurt here for uniformity\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "        \n",
    "        # Fit\n",
    "        model.fit(X_train_s, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_s)\n",
    "        \n",
    "        # Calculate Metrics (Weighted average handles class imbalance best)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test, y_pred, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def run_experiments(self):\n",
    "        \"\"\"\n",
    "        Loops through 4 Feature Sets AND Multiple Algorithms.\n",
    "        Returns a Pandas DataFrame of results.\n",
    "        \"\"\"\n",
    "        # 1. Define Feature Sets\n",
    "        feature_sets = {\n",
    "            \"Raw Accel (XYZ)\": ['x_g', 'y_g', 'z_g'],\n",
    "            \"ODBA Only\": ['odba'],\n",
    "            \"VeDBA Only\": ['vedba'],\n",
    "            \"Magnitude\": ['mag']\n",
    "        }\n",
    "        \n",
    "        # 2. Define Algorithms\n",
    "        algorithms = {\n",
    "            \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            \"SVM (RBF)\": SVC(kernel='rbf', random_state=42),\n",
    "            \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "            \"LogisticReg\": LogisticRegression(random_state=42, max_iter=1000)\n",
    "        }\n",
    "        \n",
    "        print(f\"Starting Experiments on {len(self.data)} rows...\")\n",
    "        \n",
    "        for feat_name, cols in feature_sets.items():\n",
    "            # Skip if columns missing\n",
    "            if not set(cols).issubset(self.data.columns):\n",
    "                print(f\"Skipping {feat_name} (Missing columns)\")\n",
    "                continue\n",
    "                \n",
    "            # Prepare data once per feature set\n",
    "            X_train, X_test, y_train, y_test = self.prepare_data(cols)\n",
    "            \n",
    "            for algo_name, model in algorithms.items():\n",
    "                print(f\"  -> Testing {algo_name} on {feat_name}...\")\n",
    "                \n",
    "                acc, prec, rec, f1 = self._evaluate_model(\n",
    "                    model, X_train, X_test, y_train, y_test\n",
    "                )\n",
    "                \n",
    "                # Log results\n",
    "                self.results_log.append({\n",
    "                    'Algorithm': algo_name,\n",
    "                    'Feature_Set': feat_name,\n",
    "                    'Accuracy': round(acc, 4),\n",
    "                    'Precision': round(prec, 4),\n",
    "                    'Recall': round(rec, 4),\n",
    "                    'F1_Score': round(f1, 4),\n",
    "                    'Target_Counts': self.target_distribution_str,\n",
    "                    'Features_Used': str(cols)\n",
    "                })\n",
    "                \n",
    "        # Convert to DataFrame\n",
    "        results_df = pd.DataFrame(self.results_log)\n",
    "        \n",
    "        # Sort by F1 Score to show best models first\n",
    "        return results_df.sort_values(by='F1_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c417999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk():\n",
    "        for file in files:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heat_stress_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
