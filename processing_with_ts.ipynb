{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45f7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 08:34:17.640866: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-12-14 08:34:17.668562: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-14 08:34:18.544816: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-14 08:34:20.740447: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-14 08:34:20.744489: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 263\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m zipfile.ZipFile(os.path.join(main_path, zip_file), \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zf:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m         \u001b[43mzf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m os.walk(temp_dir):\n\u001b[32m    265\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dirs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1757\u001b[39m, in \u001b[36mZipFile.extractall\u001b[39m\u001b[34m(self, path, members, pwd)\u001b[39m\n\u001b[32m   1754\u001b[39m     path = os.fspath(path)\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m zipinfo \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[32m-> \u001b[39m\u001b[32m1757\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_member\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1815\u001b[39m, in \u001b[36mZipFile._extract_member\u001b[39m\u001b[34m(self, member, targetpath, pwd)\u001b[39m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.open(member, pwd=pwd) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[32m   1814\u001b[39m      \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[32m-> \u001b[39m\u001b[32m1815\u001b[39m     \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m targetpath\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/shutil.py:203\u001b[39m, in \u001b[36mcopyfileobj\u001b[39m\u001b[34m(fsrc, fdst, length)\u001b[39m\n\u001b[32m    201\u001b[39m fsrc_read = fsrc.read\n\u001b[32m    202\u001b[39m fdst_write = fdst.write\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m buf := \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    204\u001b[39m     fdst_write(buf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1005\u001b[39m, in \u001b[36mZipExtFile.read\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28mself\u001b[39m._offset = \u001b[32m0\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m n > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m   1007\u001b[39m         \u001b[38;5;28mself\u001b[39m._readbuffer = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1081\u001b[39m, in \u001b[36mZipExtFile._read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compress_type == ZIP_DEFLATED:\n\u001b[32m   1080\u001b[39m     n = \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m.MIN_READ_SIZE)\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28mself\u001b[39m._eof = (\u001b[38;5;28mself\u001b[39m._decompressor.eof \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1083\u001b[39m                  \u001b[38;5;28mself\u001b[39m._compress_left <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m   1084\u001b[39m                  \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unconsumed_tail)\n\u001b[32m   1085\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._eof:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from scipy.signal import butter, filtfilt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# ==========================================\n",
    "# 1. ADAPTIVE PIPELINE (Handles Mixed Hz)\n",
    "# ==========================================\n",
    "class AdaptiveAccelPipeline:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df['local_ts'] = pd.to_datetime(self.df['local_ts'])\n",
    "        # Sort is critical for time-series\n",
    "        self.df = self.df.sort_values(by=['subject', 'local_ts']).reset_index(drop=True)\n",
    "\n",
    "    def apply_adaptive_filter(self, target_cutoff_hz=5.0):\n",
    "        \"\"\"\n",
    "        Applies filtering subject-by-subject.\n",
    "        If a subject's sampling rate is too low, it auto-adjusts or skips filtering.\n",
    "        \"\"\"\n",
    "        print(f\"--- Running Adaptive Filter (Target Cutoff: {target_cutoff_hz}Hz) ---\")\n",
    "        \n",
    "        def filter_subject_data(group):\n",
    "            # 1. Calculate Sampling Rate (fs) for THIS subject\n",
    "            # We filter out 0.0 diffs (duplicates) to avoid divide-by-zero\n",
    "            time_diffs = group['local_ts'].diff().dt.total_seconds()\n",
    "            valid_diffs = time_diffs[time_diffs > 0]\n",
    "            \n",
    "            if len(valid_diffs) < 10:\n",
    "                return group # Too little data to process\n",
    "            \n",
    "            median_diff = valid_diffs.median()\n",
    "            if median_diff == 0: return group # Safety catch\n",
    "            \n",
    "            fs = 1 / median_diff\n",
    "            nyquist = 0.5 * fs\n",
    "            \n",
    "            # 2. DECISION LOGIC\n",
    "            # Case A: Data is too slow for the requested filter (e.g., Data is 5Hz, you want 5Hz cut)\n",
    "            if fs <= target_cutoff_hz * 2.0:\n",
    "                # If data is really slow (<10Hz), usually we just skip filtering \n",
    "                # because it's already \"smooth\" compared to 20Hz data.\n",
    "                # Or we apply a very mild smoothing (e.g. 0.8 * Nyquist)\n",
    "                actual_cutoff = nyquist * 0.9 \n",
    "                \n",
    "                # If the resulting cutoff is tiny, just skip it.\n",
    "                if actual_cutoff < 1.0:\n",
    "                    return group \n",
    "            else:\n",
    "                # Case B: Data is fast enough (e.g., 20Hz data, 5Hz cut) -> Use requested cutoff\n",
    "                actual_cutoff = target_cutoff_hz\n",
    "\n",
    "            # 3. Apply Filter\n",
    "            try:\n",
    "                b, a = butter(N=4, Wn=actual_cutoff / nyquist, btype='low')\n",
    "                # Apply to X, Y, Z\n",
    "                for col in ['x', 'y', 'z']:\n",
    "                    group[col] = filtfilt(b, a, group[col])\n",
    "            except Exception as e:\n",
    "                # If math fails (e.g., unstable IIR), return raw data\n",
    "                pass\n",
    "                \n",
    "            return group\n",
    "\n",
    "        # Apply the logic to each subject independently\n",
    "        self.df = self.df.groupby('subject', group_keys=False).apply(filter_subject_data)\n",
    "        print(\"--- Adaptive Filtering Complete ---\")\n",
    "        return self.df\n",
    "\n",
    "    def convert_to_gravity(self):\n",
    "        print(\"--- Converting to Gravity & ENMO ---\")\n",
    "        scale = 16384.0 # Adjust based on your accelerometer range (e.g. +/- 2g vs +/- 4g)\n",
    "        self.df['x_g'] = self.df['x'] / scale\n",
    "        self.df['y_g'] = self.df['y'] / scale\n",
    "        self.df['z_g'] = self.df['z'] / scale\n",
    "        self.df['mag'] = np.sqrt(self.df['x_g']**2 + self.df['y_g']**2 + self.df['z_g']**2)\n",
    "        self.df['enmo'] = np.maximum(self.df['mag'] - 1, 0)\n",
    "        return self.df\n",
    "\n",
    "    def calc_odba(self):\n",
    "        # Simplified ODBA calculation\n",
    "        self.df['odba'] = (self.df['x_g'] - self.df['x_g'].mean()).abs() + \\\n",
    "                          (self.df['y_g'] - self.df['y_g'].mean()).abs() + \\\n",
    "                          (self.df['z_g'] - self.df['z_g'].mean()).abs()\n",
    "        return self.df\n",
    "\n",
    "    def resample_and_label(self, interval_seconds=10, coherence_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Resamples data into windows. \n",
    "        ASSIGN LABELS based on threshold:\n",
    "        If > 70% of the raw samples in the window are 'Grazing', the window is 'Grazing'.\n",
    "        Otherwise, the window is discarded (Ambiguous).\n",
    "        \"\"\"\n",
    "        print(f\"--- Resampling ({interval_seconds}s) with Threshold {coherence_threshold*100}% ---\")\n",
    "        \n",
    "        # Custom Aggregator for Labels\n",
    "        def threshold_labeler(x):\n",
    "            if x.empty: return np.nan\n",
    "            counts = x.value_counts(normalize=True)\n",
    "            # Check if the most frequent label crosses the threshold\n",
    "            if counts.iloc[0] >= coherence_threshold:\n",
    "                return counts.index[0]\n",
    "            return np.nan # Drop this window (too messy/transitioning)\n",
    "\n",
    "        # Feature Aggregators\n",
    "        agg_dict = {\n",
    "            'x_g': ['mean', 'std', 'min', 'max'],\n",
    "            'y_g': ['mean', 'std', 'min', 'max'],\n",
    "            'z_g': ['mean', 'std', 'min', 'max'],\n",
    "            'mag': ['mean', 'std'],      \n",
    "            'enmo': ['mean', 'max'], \n",
    "            'odba': ['mean', 'std'],    \n",
    "            'behavioral_category': threshold_labeler # <--- LOGIC APPLIED HERE\n",
    "        }\n",
    "\n",
    "        resampled = (\n",
    "            self.df.set_index('local_ts')\n",
    "            .groupby('subject')\n",
    "            .resample(f'{interval_seconds}s')\n",
    "            .agg(agg_dict)\n",
    "        )\n",
    "        \n",
    "        # Flatten columns\n",
    "        resampled.columns = [f\"{c[0]}_{c[1]}\" if c[1] else c[0] for c in resampled.columns]\n",
    "        resampled = resampled.rename(columns={'behavioral_category_threshold_labeler': 'label'})\n",
    "        \n",
    "        # DROP windows that failed the threshold check\n",
    "        final_df = resampled.dropna(subset=['label']).reset_index()\n",
    "        \n",
    "        print(f\"Generated {len(final_df)} labeled windows.\")\n",
    "        return final_df\n",
    "\n",
    "    def create_time_series_sequences(self, data, feature_cols, target_col, time_steps=5):\n",
    "        \"\"\"\n",
    "        Converts the resampled windows into sequences for LSTM.\n",
    "        Input: (N, Features) -> Output: (N, TimeSteps, Features)\n",
    "        \"\"\"\n",
    "        X_seq, y_seq = [], []\n",
    "        \n",
    "        # Group by subject to prevent sequences bleeding across different animals\n",
    "        for subject, group in data.groupby('subject'):\n",
    "            group = group.sort_values('local_ts')\n",
    "            feats = group[feature_cols].values\n",
    "            targets = group[target_col].values\n",
    "            \n",
    "            if len(group) < time_steps: continue\n",
    "            \n",
    "            # Sliding window over the WINDOWS\n",
    "            for i in range(len(group) - time_steps):\n",
    "                X_seq.append(feats[i : i + time_steps])\n",
    "                y_seq.append(targets[i + time_steps]) # Predict the label of the *next* window\n",
    "                \n",
    "        return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# ==========================================\n",
    "# 2. DEEP LEARNING ENGINE\n",
    "# ==========================================\n",
    "class TimeSeriesModeler:\n",
    "    def __init__(self, pipeline, df):\n",
    "        self.pipeline = pipeline\n",
    "        self.df = df\n",
    "        self.le = LabelEncoder()\n",
    "        \n",
    "    def run(self, time_steps=5):\n",
    "        print(f\"\\n>>> Running Time Series Model (Lookback: {time_steps} steps)\")\n",
    "        \n",
    "        # 1. Identify Feature Columns (exclude metadata)\n",
    "        feature_cols = [c for c in self.df.columns if c not in ['subject', 'local_ts', 'label']]\n",
    "        \n",
    "        # 2. Encode Labels\n",
    "        self.df['label_encoded'] = self.le.fit_transform(self.df['label'])\n",
    "        num_classes = len(self.le.classes_)\n",
    "        \n",
    "        # 3. Scale Features (Crucial for LSTM)\n",
    "        scaler = StandardScaler()\n",
    "        # We perform scaling on the dataframe before sequencing\n",
    "        scaled_df = self.df.copy()\n",
    "        scaled_df[feature_cols] = scaler.fit_transform(scaled_df[feature_cols])\n",
    "        \n",
    "        # 4. Generate Sequences\n",
    "        X, y = self.pipeline.create_time_series_sequences(\n",
    "            scaled_df, feature_cols, 'label_encoded', time_steps\n",
    "        )\n",
    "        \n",
    "        if len(X) == 0:\n",
    "            print(\"Not enough data for sequences.\")\n",
    "            return None\n",
    "\n",
    "        # 5. One-Hot Encode Targets\n",
    "        y_cat = to_categorical(y, num_classes=num_classes)\n",
    "        \n",
    "        # 6. Train/Test Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # 7. Define LSTM Model\n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "            LSTM(64, return_sequences=False),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # 8. Train\n",
    "        es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        history = model.fit(\n",
    "            X_train, y_train, \n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=20, \n",
    "            batch_size=32, \n",
    "            callbacks=[es],\n",
    "            verbose=0 # Silent training\n",
    "        )\n",
    "        \n",
    "        # 9. Evaluate\n",
    "        y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'Accuracy': acc,\n",
    "            'F1_Score': f1,\n",
    "            'Time_Steps': time_steps,\n",
    "            'Num_Training_Sequences': len(X_train)\n",
    "            \n",
    "        }\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# A. LOAD DATA\n",
    "actual_file = []\n",
    "main_path = \"/home/rajesh/work/acclerometer_project/zip_data\"\n",
    "\n",
    "if os.path.exists(main_path):\n",
    "    for zip_file in os.listdir(main_path):\n",
    "        if zip_file.endswith(\".zip\"):\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                try:\n",
    "                    with zipfile.ZipFile(os.path.join(main_path, zip_file), \"r\") as zf:\n",
    "                        zf.extractall(temp_dir)\n",
    "                        for root, dirs, files in os.walk(temp_dir):\n",
    "                            for d in dirs:\n",
    "                                if d.startswith('Processed'):\n",
    "                                    sec_path = os.path.join(root, d)\n",
    "                                    for f in os.listdir(sec_path):\n",
    "                                        if f.endswith(('.xls', '.xlsx')):\n",
    "                                            actual_file.append(pd.read_excel(os.path.join(sec_path, f)))\n",
    "                except Exception as e: print(f\"Error: {e}\")\n",
    "    if actual_file: df_raw = pd.concat(actual_file)\n",
    "    else: df_raw = pd.DataFrame() # Handle empty case\n",
    "else:\n",
    "    # Dummy data for testing if path doesn't exist\n",
    "    print(\"Using Dummy Data\")\n",
    "    dates = pd.date_range('2023-01-01', periods=5000, freq='200ms') # 5Hz data\n",
    "    df_raw = pd.DataFrame({\n",
    "        'subject': ['Cow1']*5000,\n",
    "        'local_ts': dates,\n",
    "        'x': np.random.randn(5000)*1000, \n",
    "        'y': np.random.randn(5000)*1000,\n",
    "        'z': np.random.randn(5000)*1000,\n",
    "        'behavioral_category': ['Grazing']*5000\n",
    "    })\n",
    "\n",
    "if not df_raw.empty:\n",
    "    # B. INITIALIZE ADAPTIVE PIPELINE\n",
    "    pipeline = AdaptiveAccelPipeline(df_raw)\n",
    "\n",
    "    # 1. Adaptively Filter (safe for mixed 5Hz/10Hz/20Hz)\n",
    "    pipeline.apply_adaptive_filter(target_cutoff_hz=5.0)\n",
    "\n",
    "    # 2. Physics conversions\n",
    "    pipeline.convert_to_gravity()\n",
    "    pipeline.calc_odba()\n",
    "\n",
    "    results_table = []\n",
    "    \n",
    "    # Grid Search Settings\n",
    "    intervals = [10, 30]  # Window sizes in seconds\n",
    "    thresholds = [0.6, 0.8] # Strictness of labeling (60% vs 80% purity)\n",
    "    lstm_steps = [5, 10]    # How far back the LSTM looks\n",
    "\n",
    "    print(f\"\\n--- Starting Grid Search ---\")\n",
    "    for iv in intervals:\n",
    "        for th in thresholds:\n",
    "            # 3. Resample and Label\n",
    "            # This applies the logic: \"Only keep window if > X% of data matches\"\n",
    "            df_ready = pipeline.resample_and_label(interval_seconds=iv, coherence_threshold=th)\n",
    "            \n",
    "            if len(df_ready) < 100:\n",
    "                print(f\"Skipping Interval={iv}, Threshold={th} (Not enough valid windows)\")\n",
    "                continue\n",
    "\n",
    "            # 4. Run Time Series Model\n",
    "            ts_modeler = TimeSeriesModeler(pipeline, df_ready)\n",
    "            \n",
    "            for steps in lstm_steps:\n",
    "                res = ts_modeler.run(time_steps=steps)\n",
    "                if res:\n",
    "                    res['Window_Size_Sec'] = iv\n",
    "                    res['Label_Threshold'] = th\n",
    "                    results_table.append(res)\n",
    "\n",
    "    # C. SAVE RESULTS\n",
    "    if results_table:\n",
    "        final_df = pd.DataFrame(results_table)\n",
    "        final_df = final_df.sort_values(by='F1_Score', ascending=False)\n",
    "        \n",
    "        print(\"\\n=== FINAL RESULTS ===\")\n",
    "        print(final_df)\n",
    "        \n",
    "        save_path = '/home/rajesh/work/acclerometer_project/codes/timeseries_results.csv'\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        final_df.to_csv(save_path, index=False)\n",
    "    else:\n",
    "        print(\"No results generated.\")\n",
    "else:\n",
    "    print(\"No Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ced191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Time_Steps</th>\n",
       "      <th>Num_Training_Sequences</th>\n",
       "      <th>Window_Size_Sec</th>\n",
       "      <th>Label_Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.929608</td>\n",
       "      <td>0.928017</td>\n",
       "      <td>10</td>\n",
       "      <td>14888</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915121</td>\n",
       "      <td>0.913842</td>\n",
       "      <td>10</td>\n",
       "      <td>15736</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.904425</td>\n",
       "      <td>0.900703</td>\n",
       "      <td>10</td>\n",
       "      <td>4516</td>\n",
       "      <td>30</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.877276</td>\n",
       "      <td>0.870692</td>\n",
       "      <td>10</td>\n",
       "      <td>5049</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.857603</td>\n",
       "      <td>0.852966</td>\n",
       "      <td>5</td>\n",
       "      <td>14916</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.847845</td>\n",
       "      <td>0.842795</td>\n",
       "      <td>5</td>\n",
       "      <td>4544</td>\n",
       "      <td>30</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831304</td>\n",
       "      <td>0.824163</td>\n",
       "      <td>5</td>\n",
       "      <td>15764</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.829134</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>5</td>\n",
       "      <td>5077</td>\n",
       "      <td>30</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  F1_Score  Time_Steps  Num_Training_Sequences  Window_Size_Sec  \\\n",
       "3  0.929608  0.928017          10                   14888               10   \n",
       "1  0.915121  0.913842          10                   15736               10   \n",
       "7  0.904425  0.900703          10                    4516               30   \n",
       "5  0.877276  0.870692          10                    5049               30   \n",
       "2  0.857603  0.852966           5                   14916               10   \n",
       "6  0.847845  0.842795           5                    4544               30   \n",
       "0  0.831304  0.824163           5                   15764               10   \n",
       "4  0.829134  0.821685           5                    5077               30   \n",
       "\n",
       "   Label_Threshold  \n",
       "3              0.8  \n",
       "1              0.6  \n",
       "7              0.8  \n",
       "5              0.6  \n",
       "2              0.8  \n",
       "6              0.8  \n",
       "0              0.6  \n",
       "4              0.6  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbf46f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heat_stress_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
